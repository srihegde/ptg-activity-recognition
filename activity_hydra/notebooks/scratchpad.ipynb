{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17edb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def import_modules(libpath):\n",
    "    path2add = os.path.normpath(os.path.abspath(os.path.join(os.path.dirname(\n",
    "        libpath), os.path.pardir)))\n",
    "    print(f'Adding path: {path2add}')\n",
    "    if (not (path2add in sys.path)):\n",
    "        sys.path.append(path2add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c35490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding path: /data/sri.hegde/ptg-activity-recognition/activity_hydra/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khq.kitware.com/sri.hegde/miniconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from torchvision.models.feature_extraction import create_feature_extractor\n",
    "# from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "import_modules('../src/models/components')\n",
    "from datamodules.components.frame_dataset import H2OFrameDataset\n",
    "from models.components.unified_fcn import UnifiedFCNModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the datahandler/dataloader implementation\n",
    "dataset = H2OFrameDataset(\n",
    "    '../data/h2o/', '../data/h2o/label_split/pose_train.txt')\n",
    "data = dataset[2]\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13167101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions to create grid data\n",
    "\n",
    "def conf_func(dist, alpha, dth):\n",
    "    dist = np.sqrt(np.sum((dist)**2, axis=-1))\n",
    "    mask = (dist < dth)\n",
    "    conf = np.exp(alpha*(1 - dist/dth))\n",
    "    conf = mask * conf\n",
    "    mean_conf = np.mean(conf, axis=-1)\n",
    "    return mean_conf\n",
    "\n",
    "\n",
    "def corner_confidences(cp_pred_np: np.ndarray, obj_pose: np.ndarray, l_hand: np.ndarray, r_hand: np.ndarray, alpha: float = 2.0, dth=[75, 75, 7.5]):\n",
    "    cp_gt = np.stack([obj_pose, l_hand, r_hand])\n",
    "    cp_gt = cp_gt.reshape(cp_gt.shape[:-1] + (-1, 3))\n",
    "    cp_pred_np = cp_pred_np.reshape(cp_pred_np.shape[:-1] + (-1, 3))\n",
    "    dist = cp_gt - cp_pred_np[:, :, ]\n",
    "    c_uv = conf_func(dist[..., :2], alpha, dth[0])\n",
    "    z_mask = (dist[..., -1] < dth[-1])\n",
    "    c_z = np.mean(z_mask * np.abs(dist[..., -1]), axis=-1)\n",
    "    conf = 0.5*(c_uv + c_z)\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f11a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the working of Unified FCN\n",
    "ufcn = UnifiedFCNModule('convnext_tiny', 21, 9, 12)\n",
    "net = ufcn.net\n",
    "train_nodes, _ = get_graph_node_names(net)\n",
    "\n",
    "net = create_feature_extractor(\n",
    "    net, return_nodes={'features.7.2.block.4': 'feat_out'})\n",
    "out = net(torch.rand(1, 3, 416, 416))\n",
    "x = out['feat_out']\n",
    "out_channels = 5 * 3 * (3 * ufcn.num_cpts + 1 +\n",
    "                        ufcn.obj_classes + ufcn.verb_classes)\n",
    "lin = nn.Linear(x.shape[-1], out_channels)\n",
    "x = lin(x)\n",
    "# x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "bsize, _, h, w = x.size()\n",
    "x_reshaped = x.contiguous().view(bsize, -1, 3, 3 * ufcn.num_cpts +\n",
    "                                 1 + ufcn.obj_classes + ufcn.verb_classes)\n",
    "# print(x.shape)\n",
    "\n",
    "# vector indices (at position 2): 0 -> object, 1 -> l_hand, 2 -> r_hand\n",
    "cp_pred = torch.sigmoid(x_reshaped[:, :, :, 0:3 * ufcn.num_cpts])\n",
    "conf_pred = x_reshaped[:, :, :, 3 * ufcn.num_cpts].contiguous()\n",
    "obj_pred = torch.sigmoid(\n",
    "    x_reshaped[:, :, 0, 3 * ufcn.num_cpts+1: 3 * ufcn.num_cpts+1+ufcn.obj_classes])\n",
    "l_verb_pred = torch.sigmoid(x_reshaped[:, :, 1, 3 * ufcn.num_cpts+1 +\n",
    "                            ufcn.obj_classes: 3 * ufcn.num_cpts+1+ufcn.obj_classes+ufcn.verb_classes])\n",
    "r_verb_pred = torch.sigmoid(x_reshaped[:, :, 2, 3 * ufcn.num_cpts+1 +\n",
    "                            ufcn.obj_classes: 3 * ufcn.num_cpts+1+ufcn.obj_classes+ufcn.verb_classes])\n",
    "\n",
    "print(cp_pred.shape, conf_pred.shape, obj_pred.shape,\n",
    "      l_verb_pred.shape, r_verb_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96620180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence computation\n",
    "l_hand, r_hand, obj_label, obj_pose, verb = data['l_hand'], data[\n",
    "    'r_hand'], data['obj_label'], data['obj_pose'], data['verb']\n",
    "conf = corner_confidences(cp_pred.data.cpu().numpy(), obj_pose, l_hand, r_hand)\n",
    "# print(conf.shape)\n",
    "\n",
    "noho_scale = 0.1\n",
    "ho_scale = 5\n",
    "conf_mask = np.ones_like(conf)*noho_scale\n",
    "print(conf_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063c301",
   "metadata": {},
   "source": [
    "# Visualize predicted and GT bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca692068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "gt = [{'gt_num': 1, 'gt_boxes_upright_depth': [[0.0608, 0.0426, 0.5437, 0.1157, 0.1120, 0.1126]], 'class': [8]}, {'gt_num': 1, 'gt_boxes_upright_depth': [[0.0367, 0.0234, 0.5902, 0.1157, 0.1120, 0.1126]], 'class': [8]}, {'gt_num': 1, 'gt_boxes_upright_depth': [[0.0276, 0.0230, 0.6005, 0.1157, 0.1120, 0.1126]], 'class': [8]}, {'gt_num': 1, 'gt_boxes_upright_depth': [[0.0098, 0.0136, 0.5946, 0.1157, 0.1120, 0.1126]], 'class': [8]}, {'gt_num': 1, 'gt_boxes_upright_depth': [[0.0230, 0.0301, 0.6039, 0.1157, 0.1120, 0.1126]], 'class': [8]}]\n",
    "results = [{'boxes_3d': [[ 4.2602e-03,  5.2471e-04,  4.9059e-02,  4.1665e-02,  6.8070e-02,\n",
    "          3.5510e-02,  6.3585e+00],\n",
    "        [ 2.7554e-03, -3.2314e-04,  3.0911e-02,  2.0331e-02,  4.1466e-02,\n",
    "          1.7244e-02,  6.3778e+00],\n",
    "        [ 1.4760e-03,  2.7081e-04,  2.2068e-02,  1.7085e-02,  3.7264e-02,\n",
    "          1.5370e-02,  6.4259e+00],\n",
    "        [ 2.5878e-03,  2.4019e-04,  3.7491e-02,  3.5269e-02,  6.2041e-02,\n",
    "          2.9389e-02,  6.4114e+00],\n",
    "        [ 1.8443e-03, -5.7145e-04,  3.1550e-02,  1.2864e-02,  3.4855e-02,\n",
    "          1.2427e-02,  6.4068e+00],\n",
    "        [ 4.8076e-03,  1.5290e-03,  4.0302e-02,  3.0070e-02,  5.4107e-02,\n",
    "          2.4297e-02,  6.3619e+00],\n",
    "        [ 1.2358e-02,  2.9496e-03,  6.4462e-02,  5.6419e-02,  8.6110e-02,\n",
    "          4.8433e-02,  6.4080e+00],\n",
    "        [ 5.3392e-03, -9.3258e-04,  4.8271e-02,  2.3491e-02,  4.8560e-02,\n",
    "          2.0297e-02,  6.3691e+00],\n",
    "        [-1.4695e-03,  6.3801e-04,  2.7495e-02,  2.1895e-02,  4.6697e-02,\n",
    "          2.0390e-02,  6.3689e+00],\n",
    "        [-3.0802e-03,  2.0738e-03,  3.9001e-02,  4.4559e-02,  7.7915e-02,\n",
    "          3.7976e-02,  6.3573e+00],\n",
    "        [-2.4542e-03, -1.1351e-03,  4.2858e-02,  1.8496e-02,  4.5568e-02,\n",
    "          1.8163e-02,  6.3802e+00],\n",
    "        [ 9.9102e-03,  2.5398e-02,  9.2619e-02,  9.9285e-02,  1.4596e-01,\n",
    "          9.9848e-02,  6.3094e+00]], 'scores_3d': [0.2975, 0.2823, 0.2410, 0.2406, 0.1858, 0.1828, 0.1757, 0.1128, 0.1045,\n",
    "        0.0875, 0.0748, 0.0644], 'labels_3d': [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]}, {'boxes_3d': [[ 3.5143e-03,  1.1140e-03,  4.5588e-02,  4.4120e-02,  7.2670e-02,\n",
    "          3.8275e-02,  6.3660e+00],\n",
    "        [ 2.2898e-03, -3.3071e-04,  2.5120e-02,  2.2491e-02,  4.4327e-02,\n",
    "          1.8669e-02,  6.3837e+00],\n",
    "        [ 2.4653e-03,  6.7453e-04,  3.4972e-02,  3.6590e-02,  6.1495e-02,\n",
    "          3.0332e-02,  6.4175e+00],\n",
    "        [ 1.2567e-03,  2.1678e-04,  2.1562e-02,  1.6043e-02,  3.5033e-02,\n",
    "          1.3773e-02,  6.4222e+00],\n",
    "        [ 1.3964e-03,  6.7389e-05,  3.0861e-02,  1.2121e-02,  3.3254e-02,\n",
    "          1.1411e-02,  6.4274e+00],\n",
    "        [-1.5366e-03,  6.7237e-04,  2.6178e-02,  2.0999e-02,  4.5943e-02,\n",
    "          1.9971e-02,  6.3779e+00],\n",
    "        [ 6.3550e-03,  1.2876e-03,  3.6886e-02,  3.2408e-02,  5.7179e-02,\n",
    "          2.7509e-02,  6.4173e+00],\n",
    "        [ 1.4005e-02,  4.3267e-03,  6.6092e-02,  5.8352e-02,  8.8441e-02,\n",
    "          4.9612e-02,  6.4718e+00],\n",
    "        [-2.4569e-03,  1.6528e-03,  4.0393e-02,  4.2257e-02,  7.3554e-02,\n",
    "          3.5196e-02,  6.3795e+00],\n",
    "        [ 7.5612e-03, -1.5335e-03,  5.1426e-02,  2.4108e-02,  5.0948e-02,\n",
    "          2.1128e-02,  6.4236e+00],\n",
    "        [ 9.2552e-03,  2.7353e-02,  9.4263e-02,  1.0368e-01,  1.5013e-01,\n",
    "          9.8138e-02,  6.3014e+00]], 'scores_3d': [0.2935, 0.2829, 0.2604, 0.2549, 0.1933, 0.1420, 0.1240, 0.1172, 0.1156,\n",
    "        0.0876, 0.0665], 'labels_3d': [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]}, {'boxes_3d': [[ 2.2215e-03,  1.2342e-03,  4.5963e-02,  4.5802e-02,  7.3616e-02,\n",
    "          3.8791e-02,  6.3598e+00],\n",
    "        [ 1.5422e-03,  1.9453e-04,  2.8852e-02,  2.2254e-02,  4.3009e-02,\n",
    "          1.7833e-02,  6.3638e+00],\n",
    "        [ 1.7672e-03,  2.2735e-04,  3.6136e-02,  3.7316e-02,  6.3868e-02,\n",
    "          3.1232e-02,  6.4154e+00],\n",
    "        [ 8.3405e-04,  1.8281e-04,  2.1335e-02,  1.7564e-02,  3.7569e-02,\n",
    "          1.5221e-02,  6.4088e+00],\n",
    "        [ 7.8722e-04,  3.1088e-04,  3.0859e-02,  1.2647e-02,  3.4672e-02,\n",
    "          1.2047e-02,  6.4058e+00],\n",
    "        [-1.8676e-03,  5.3797e-04,  2.4578e-02,  2.0148e-02,  4.3895e-02,\n",
    "          1.9182e-02,  6.3849e+00],\n",
    "        [-3.0614e-03,  1.5974e-03,  3.7432e-02,  4.2867e-02,  7.4040e-02,\n",
    "          3.6636e-02,  6.3648e+00],\n",
    "        [ 6.1169e-03,  2.0373e-03,  4.1623e-02,  3.1713e-02,  5.5458e-02,\n",
    "          2.6231e-02,  6.4310e+00],\n",
    "        [ 1.4327e-02,  4.4097e-03,  7.1036e-02,  6.0509e-02,  8.9436e-02,\n",
    "          5.1614e-02,  6.4688e+00],\n",
    "        [ 7.6242e-03, -1.4671e-03,  5.1131e-02,  2.4165e-02,  5.0639e-02,\n",
    "          2.1695e-02,  6.4447e+00],\n",
    "        [ 6.9473e-03,  2.8553e-02,  1.0006e-01,  1.0847e-01,  1.5243e-01,\n",
    "          1.0024e-01,  6.2842e+00]], 'scores_3d': [0.2967, 0.2812, 0.2659, 0.2577, 0.1947, 0.1489, 0.1246, 0.1203, 0.1050,\n",
    "        0.0831, 0.0618], 'labels_3d': [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]}, {'boxes_3d': [[ 2.0585e-03, -3.7776e-04,  2.9671e-02,  2.2234e-02,  4.3007e-02,\n",
    "          1.7865e-02,  6.4333e+00],\n",
    "        [ 3.7915e-03,  7.6625e-04,  4.8518e-02,  4.4233e-02,  7.3481e-02,\n",
    "          3.7787e-02,  6.4017e+00],\n",
    "        [ 2.1777e-03,  2.7639e-04,  3.5109e-02,  3.7682e-02,  6.4989e-02,\n",
    "          3.1578e-02,  6.4383e+00],\n",
    "        [ 1.3203e-03, -1.8006e-04,  2.1038e-02,  1.6179e-02,  3.5543e-02,\n",
    "          1.4186e-02,  6.4761e+00],\n",
    "        [ 1.1089e-03, -4.8889e-04,  3.0157e-02,  1.2362e-02,  3.4118e-02,\n",
    "          1.1559e-02,  6.4289e+00],\n",
    "        [-1.1203e-03,  1.4172e-04,  2.3592e-02,  1.9768e-02,  4.4702e-02,\n",
    "          1.9158e-02,  6.4054e+00],\n",
    "        [ 6.6975e-03,  9.2305e-04,  3.9167e-02,  3.2537e-02,  5.7294e-02,\n",
    "          2.6770e-02,  6.4495e+00],\n",
    "        [-2.8697e-03,  1.5351e-03,  3.8210e-02,  4.4426e-02,  7.5343e-02,\n",
    "          3.7498e-02,  6.4079e+00],\n",
    "        [-1.3871e-03, -1.2734e-03,  3.8744e-02,  1.6143e-02,  4.1936e-02,\n",
    "          1.6370e-02,  6.4024e+00],\n",
    "        [ 1.5768e-02,  4.9125e-03,  7.1725e-02,  6.4920e-02,  9.5957e-02,\n",
    "          5.5656e-02,  6.4698e+00],\n",
    "        [ 7.2098e-03, -9.0235e-04,  4.8401e-02,  2.3512e-02,  4.8862e-02,\n",
    "          2.0625e-02,  6.4217e+00],\n",
    "        [ 9.3729e-03,  3.0865e-02,  1.0556e-01,  1.0813e-01,  1.5269e-01,\n",
    "          1.0352e-01,  6.3016e+00]], 'scores_3d': [0.2836, 0.2812, 0.2607, 0.2582, 0.1879, 0.1539, 0.1176, 0.1167, 0.1031,\n",
    "        0.1023, 0.0912, 0.0576], 'labels_3d': [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]}, {'boxes_3d': [[ 2.3924e-03,  1.0150e-03,  4.5687e-02,  4.5438e-02,  7.3256e-02,\n",
    "          3.8520e-02,  6.3666e+00],\n",
    "        [ 1.5279e-03,  6.0028e-04,  2.7723e-02,  2.2595e-02,  4.4020e-02,\n",
    "          1.8334e-02,  6.3806e+00],\n",
    "        [ 1.4771e-03,  1.4006e-05,  3.5330e-02,  3.5814e-02,  6.1235e-02,\n",
    "          2.9381e-02,  6.4096e+00],\n",
    "        [ 6.3927e-04,  3.8336e-04,  2.0594e-02,  1.6960e-02,  3.6159e-02,\n",
    "          1.4450e-02,  6.4069e+00],\n",
    "        [ 4.2850e-04,  5.8526e-04,  2.9921e-02,  1.2704e-02,  3.5004e-02,\n",
    "          1.2056e-02,  6.3828e+00],\n",
    "        [-2.0349e-03,  5.9369e-04,  2.3466e-02,  1.9317e-02,  4.2250e-02,\n",
    "          1.8173e-02,  6.3765e+00],\n",
    "        [-3.0615e-03,  1.4443e-03,  3.4656e-02,  4.2257e-02,  7.3143e-02,\n",
    "          3.5784e-02,  6.3724e+00],\n",
    "        [ 6.5856e-03,  2.2203e-03,  4.2721e-02,  3.2581e-02,  5.6871e-02,\n",
    "          2.6687e-02,  6.4282e+00],\n",
    "        [ 1.4278e-02,  4.6393e-03,  7.1333e-02,  6.3113e-02,  9.1646e-02,\n",
    "          5.4669e-02,  6.4552e+00],\n",
    "        [ 8.4494e-03, -1.5283e-03,  5.5426e-02,  2.5070e-02,  5.2540e-02,\n",
    "          2.2768e-02,  6.4366e+00],\n",
    "        [ 7.5856e-03,  2.7500e-02,  9.6643e-02,  1.0626e-01,  1.5090e-01,\n",
    "          9.9937e-02,  6.2948e+00]], 'scores_3d': [0.2976, 0.2846, 0.2724, 0.2543, 0.1893, 0.1558, 0.1299, 0.1096, 0.1009,\n",
    "        0.0812, 0.0598], 'labels_3d': [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418fa51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuboid_data(center, size):\n",
    "    '''\n",
    "    Create a data array for cuboid plotting.\n",
    "\n",
    "\n",
    "    ============= ================================================\n",
    "    Argument      Description\n",
    "    ============= ================================================\n",
    "    center        center of the cuboid, triple\n",
    "    size          size of the cuboid, triple, (x_length,y_width,z_height)\n",
    "    :type size: tuple, numpy.array, list\n",
    "    :param size: size of the cuboid, triple, (x_length,y_width,z_height)\n",
    "    :type center: tuple, numpy.array, list\n",
    "    :param center: center of the cuboid, triple, (x,y,z)\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    # suppose axis direction: x: to left; y: to inside; z: to upper\n",
    "    # get the (left, outside, bottom) point\n",
    "    o = [a - b / 2 for a, b in zip(center, size)]\n",
    "    # get the length, width, and height\n",
    "    l, w, h = size\n",
    "    x = np.array([[o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in bottom surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in upper surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in outside surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]]])  # x coordinate of points in inside surface\n",
    "    y = np.array([[o[1], o[1], o[1] + w, o[1] + w, o[1]],  # y coordinate of points in bottom surface\n",
    "         [o[1], o[1], o[1] + w, o[1] + w, o[1]],  # y coordinate of points in upper surface\n",
    "         [o[1], o[1], o[1], o[1], o[1]],          # y coordinate of points in outside surface\n",
    "         [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]])    # y coordinate of points in inside surface\n",
    "    z = np.array([[o[2], o[2], o[2], o[2], o[2]],                        # z coordinate of points in bottom surface\n",
    "         [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],    # z coordinate of points in upper surface\n",
    "         [o[2], o[2], o[2] + h, o[2] + h, o[2]],                # z coordinate of points in outside surface\n",
    "         [o[2], o[2], o[2] + h, o[2] + h, o[2]]])                # z coordinate of points in inside surface\n",
    "    return x, y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae274db",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(gt)\n",
    "for i in range(N):\n",
    "    gt_box = np.array(gt[i]['gt_boxes_upright_depth'])\n",
    "    pred_box = np.array(results[i]['boxes_3d'])[:,:6]\n",
    "    \n",
    "    all_boxes = np.concatenate([gt_box, pred_box])\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    for b in range(len(all_boxes)):\n",
    "        X, Y, Z = cuboid_data(all_boxes[b][:3], (all_boxes[b][3], all_boxes[b][4], all_boxes[b][5]))\n",
    "        if b == 0:\n",
    "            col = 'r'\n",
    "        else:\n",
    "            col = 'b'\n",
    "        ax.plot_surface(X, Y, Z, color=col, rstride=1, cstride=1, alpha=0.1)\n",
    "#     ax.set_xlabel('X')\n",
    "#     ax.set_xlim(-100, 100)\n",
    "#     ax.set_ylabel('Y')\n",
    "#     ax.set_ylim(-100, 100)\n",
    "#     ax.set_zlabel('Z')\n",
    "#     ax.set_zlim(-100, 100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8eabb5",
   "metadata": {},
   "source": [
    "# Pre-computation of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ed2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnext50_32x4d\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d34ec06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                            | 0/23391 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/khq.kitware.com/sri.hegde/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/khq.kitware.com/sri.hegde/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/khq.kitware.com/sri.hegde/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/data/sri.hegde/ptg-activity-recognition/activity_hydra/src/datamodules/components/frame_dataset.py\", line 102, in __getitem__\n    verb = self._load_verb(os.path.join(path_list, \"verb_label\", f\"{fname}.txt\"))\n  File \"/data/sri.hegde/ptg-activity-recognition/activity_hydra/src/datamodules/components/frame_dataset.py\", line 75, in _load_verb\n    with open(annotation_file) as f:\nFileNotFoundError: [Errno 2] No such file or directory: '../data/h2o/subject4/h1/0/cam4/verb_label/000000.txt'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     11\u001b[0m             dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     12\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m             shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataloader))\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataloader):\n\u001b[1;32m     20\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_label\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1250\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/khq.kitware.com/sri.hegde/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/khq.kitware.com/sri.hegde/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/khq.kitware.com/sri.hegde/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/data/sri.hegde/ptg-activity-recognition/activity_hydra/src/datamodules/components/frame_dataset.py\", line 102, in __getitem__\n    verb = self._load_verb(os.path.join(path_list, \"verb_label\", f\"{fname}.txt\"))\n  File \"/data/sri.hegde/ptg-activity-recognition/activity_hydra/src/datamodules/components/frame_dataset.py\", line 75, in _load_verb\n    with open(annotation_file) as f:\nFileNotFoundError: [Errno 2] No such file or directory: '../data/h2o/subject4/h1/0/cam4/verb_label/000000.txt'\n"
     ]
    }
   ],
   "source": [
    "net = UnifiedFCNModule(\"resnext\", 21, 9, 12).to('cuda')\n",
    "trans = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ]\n",
    "            )\n",
    "root_path = \"../data/h2o/\"\n",
    "dataset = H2OFrameDataset(root_path, '../data/h2o/label_split/pose_test.txt', trans)\n",
    "dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=12,\n",
    "            pin_memory=False,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "print(len(dataloader))\n",
    "for data in tqdm.tqdm(dataloader):\n",
    "    data[\"frm\"] = data[\"frm\"].to('cuda')\n",
    "    data[\"obj_label\"] = data[\"obj_label\"].to('cuda')\n",
    "    data[\"verb\"] = data[\"verb\"].to('cuda')    \n",
    "    feats, _, _ = net(data)\n",
    "    labels = {k: data[k] for k in data if k != 'frm'}\n",
    "    sample_info = dict(feats=feats, labels=labels)\n",
    "\n",
    "    fsplit = data['fname'][0].strip().split('/')\n",
    "    if not os.path.isdir('/'.join(fsplit[:-2] + ['feat'])):\n",
    "        os.mkdir('/'.join(fsplit[:-2] + ['feat']))\n",
    "    fpath = '/'.join(fsplit[:-2] + ['feat', f'{fsplit[-1].split(\".\")[0]}.pk'])\n",
    "#     print(fpath)\n",
    "    torch.save(sample_info, fpath)\n",
    "\n",
    "print(\"All training image features computed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "427ae366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feats': tensor([[0.4777, 0.3931, 0.3964,  ..., 0.4264, 0.5056, 0.4327]],\n",
       "        device='cuda:0'),\n",
       " 'labels': {'l_hand': tensor([[-0.0434,  0.0190,  0.3661, -0.0119,  0.0065,  0.3457,  0.0163, -0.0064,\n",
       "            0.3423,  0.0362, -0.0238,  0.3421,  0.0633, -0.0375,  0.3498, -0.0033,\n",
       "           -0.0620,  0.3441,  0.0158, -0.0903,  0.3511,  0.0359, -0.0929,  0.3627,\n",
       "            0.0589, -0.0922,  0.3715, -0.0127, -0.0746,  0.3658,  0.0128, -0.0926,\n",
       "            0.3777,  0.0354, -0.0928,  0.3867,  0.0596, -0.0870,  0.3936, -0.0208,\n",
       "           -0.0648,  0.3900,  0.0048, -0.0833,  0.3989,  0.0296, -0.0832,  0.4082,\n",
       "            0.0551, -0.0802,  0.4134, -0.0236, -0.0512,  0.4085, -0.0038, -0.0578,\n",
       "            0.4166,  0.0145, -0.0588,  0.4215,  0.0331, -0.0617,  0.4271]],\n",
       "         dtype=torch.float64),\n",
       "  'r_hand': tensor([[ 0.1707,  0.0855,  0.4396,  0.1569,  0.0549,  0.4156,  0.1446,  0.0255,\n",
       "            0.4153,  0.1471, -0.0008,  0.4222,  0.1413, -0.0291,  0.4407,  0.1880,\n",
       "            0.0007,  0.4151,  0.1842, -0.0302,  0.4217,  0.1692, -0.0409,  0.4327,\n",
       "            0.1501, -0.0495,  0.4413,  0.2012, -0.0050,  0.4373,  0.1957, -0.0358,\n",
       "            0.4510,  0.1767, -0.0475,  0.4607,  0.1514, -0.0526,  0.4685,  0.2027,\n",
       "            0.0059,  0.4631,  0.1997, -0.0241,  0.4718,  0.1791, -0.0380,  0.4808,\n",
       "            0.1564, -0.0440,  0.4852,  0.1978,  0.0172,  0.4828,  0.1891, -0.0015,\n",
       "            0.4900,  0.1759, -0.0155,  0.4931,  0.1624, -0.0290,  0.4958]],\n",
       "         dtype=torch.float64),\n",
       "  'obj_label': tensor([4], device='cuda:0'),\n",
       "  'obj_pose': tensor([[ 0.0631, -0.0411,  0.4119,  0.1098, -0.0120,  0.5034,  0.0941,  0.0277,\n",
       "            0.4873,  0.0639, -0.0572,  0.3064,  0.0795, -0.0969,  0.3224,  0.0624,\n",
       "           -0.0250,  0.5174,  0.0467,  0.0147,  0.5014,  0.0165, -0.0702,  0.3204,\n",
       "            0.0321, -0.1099,  0.3364,  0.1020,  0.0078,  0.4953,  0.0546, -0.0052,\n",
       "            0.5094,  0.0243, -0.0901,  0.3284,  0.0717, -0.0771,  0.3144,  0.0861,\n",
       "           -0.0185,  0.5104,  0.0704,  0.0212,  0.4944,  0.0402, -0.0637,  0.3134,\n",
       "            0.0558, -0.1034,  0.3294,  0.0946, -0.0545,  0.4129,  0.0790, -0.0148,\n",
       "            0.3968,  0.0316, -0.0278,  0.4109,  0.0472, -0.0675,  0.4269]],\n",
       "         dtype=torch.float64),\n",
       "  'verb': tensor([1], device='cuda:0'),\n",
       "  'fname': ['../data/h2o/subject3/k1/5/cam4/rgb/000792.png']}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feats = torch.load(fpath)\n",
    "all_feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
